# QA æ•°æ®é›†å¿«é€Ÿä¸Šæ‰‹

## ğŸš€ 5 åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### 1. é…ç½®æ–‡ä»¶
```yaml
# configs/my_gsm8k.yaml
dataset_settings:
  type: "qa"
  name: "qa-gsm8k"
  qa_settings:
    split:
      train: 7000
      test: 1319
    hf_path: "openai/gsm8k"
    extract_final_answer: true
```

### 2. åŠ è½½æ•°æ®é›†
```python
from engine.datas import load_dataset
from engine.configs import load_config

config = load_config("configs/my_gsm8k.yaml")
dataset_bundle = load_dataset(config)

# è®¿é—®æ•°æ®
train_ds = dataset_bundle["splits"]["train"]
test_ds = dataset_bundle["splits"]["test"]
judge = dataset_bundle["judge"]

# æ ·æœ¬ç¤ºä¾‹
sample = train_ds[0]
print(sample["question"])      # é—®é¢˜
print(sample["final_answer"])  # æœ€ç»ˆç­”æ¡ˆ
```

### 3. ä½¿ç”¨ Judge è¯„ä¼°
```python
# å•æ¡è¯„ä¼°
result = judge("3", "3")
# {"correct": 1, "total": 1, "accuracy": 1.0}

# æ‰¹é‡è¯„ä¼°
predictions = ["1", "2", "3"]
references = ["1", "5", "3"]
result = judge(predictions, references)
# {"correct": 2, "total": 3, "accuracy": 0.6667}
```

---

## ğŸ“Š æ•°æ®æ ¼å¼

### æ ·æœ¬ç»“æ„
```python
{
    "question": "A robe takes 2 bolts of blue fiber...",
    "answer": "It takes 2/2=1 bolt...\n#### 3",
    "final_answer": "3",
    "source_split": "train"
}
```

---

## ğŸ¯ Judge å‡½æ•°ç‰¹æ€§

### å®¹é”™å¤„ç†
```python
judge("3", "3")          # âœ… æ­£ç¡®
judge("3.0", "3")        # âœ… æ­£ç¡®
judge("$3", "3")         # âœ… æ­£ç¡®
judge("3,000", "3000")   # âœ… æ­£ç¡®
judge("3.14", "3")       # âŒ é”™è¯¯
```

### æ”¯æŒæ ¼å¼
- æ•´æ•°: `3`, `-5`
- å°æ•°: `3.14`, `0.5`
- å¸¦ç¬¦å·: `$100`, `-$50`
- å¸¦é€—å·: `1,000`, `10,000`

---

## âš™ï¸ é…ç½®é€‰é¡¹

```yaml
qa_settings:
  # å¿…éœ€
  split:
    train: 7000        # æˆ– 0.8 (æ¯”ä¾‹) æˆ– 'all' (å…¨éƒ¨) æˆ– -1 (å ä½)
    test: 1319
  
  # å¯é€‰
  hf_path: "openai/gsm8k"        # HF æ•°æ®é›†è·¯å¾„ï¼ˆé»˜è®¤å€¼ç”± Preparer æä¾›ï¼‰
  hf_config: "main"               # HF é…ç½®åï¼ˆå¯é€‰ï¼‰
  extract_final_answer: true      # æ˜¯å¦æå–æœ€ç»ˆç­”æ¡ˆï¼ˆé»˜è®¤ trueï¼‰
  load_splits: ["train", "test"]  # ä» HF åŠ è½½çš„åŸå§‹ splits
```

---

## ğŸ”§ æ·»åŠ æ–° QA æ•°æ®é›†

### æœ€å°å®ç°ï¼ˆ4 æ­¥ï¼‰

1ï¸âƒ£ **åˆ›å»º Preparer**
```python
# engine/datas/impl/qa/my_dataset.py
from ...base.qa import BasePreparer, QADataset

class MyDatasetPreparer(BasePreparer):
    def _load_all(self):
        # åŠ è½½æ•°æ®
        return samples
    
    def get(self):
        samples = self._load_all()
        splits, ph = self.split_samples(samples)
        meta = self.build_meta(samples, splits, ph)
        judge = self._build_judge(meta, splits)
        return {"splits": splits, "meta": meta, "judge": judge}
    
    def _build_judge(self, meta, splits):
        # è¿”å› judge å‡½æ•°
        def judge(pred, ref, sample=None):
            ...
        return judge
```

2ï¸âƒ£ **å¯¼å‡º**
```python
# engine/datas/impl/qa/__init__.py
from .my_dataset import MyDatasetPreparer  # noqa: F401
```

3ï¸âƒ£ **æ³¨å†Œ**
```python
# engine/datas/loader.py
DATASET_REGISTRY = {
    "qa-mydataset": MyDatasetPreparer,
}
```

4ï¸âƒ£ **ä½¿ç”¨**
```yaml
dataset_settings:
  type: "qa"
  name: "qa-mydataset"
```

---

## ğŸ“ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•ä½¿ç”¨ math_verifyï¼Ÿ
A: å®‰è£… `pip install math-verify`ï¼ŒGSM8K Preparer ä¼šè‡ªåŠ¨ä½¿ç”¨ã€‚å¦‚æœæœªå®‰è£…ï¼Œä¼šè‡ªåŠ¨é™çº§åˆ°æ•°å€¼æ¯”è¾ƒã€‚

### Q: å¦‚ä½•æå–ä¸åŒæ ¼å¼çš„ç­”æ¡ˆï¼Ÿ
A: åœ¨åŸºç±»ä¸­è°ƒç”¨ `extract_answer(text, format_type="gsm8k")` æˆ– `format_type="math"`ã€‚

### Q: å¦‚ä½•æ·»åŠ è‡ªå®šä¹‰åˆ¤å®šé€»è¾‘ï¼Ÿ
A: åœ¨å­ç±»çš„ `_build_judge()` æ–¹æ³•ä¸­å®ç°è‡ªå®šä¹‰é€»è¾‘ã€‚

### Q: æ”¯æŒå“ªäº› split é…ç½®ï¼Ÿ
A: 
- æ¯”ä¾‹: `0.8` (80%)
- ç»å¯¹æ•°: `7000`
- å…¨éƒ¨: `'all'`
- å ä½: `-1`

---

## ğŸ“ ç¤ºä¾‹ï¼šMATH æ•°æ®é›†

```python
# engine/datas/impl/qa/math.py
class MATHPreparer(BasePreparer):
    def __init__(self, config):
        super().__init__(config)
        self.hf_path = "hendrycks/math"
    
    def _load_all(self):
        dataset = load_dataset(self.hf_path, split="train")
        samples = []
        for item in dataset:
            samples.append({
                "question": item["problem"],
                "answer": item["solution"],
                "final_answer": self.extract_answer(
                    item["solution"], 
                    format_type="math"  # \boxed{} æ ¼å¼
                ),
                "source_split": "train",
            })
        return samples
```

æ³¨å†Œåå³å¯ä½¿ç”¨ï¼š
```yaml
dataset_settings:
  type: "qa"
  name: "qa-math"
  qa_settings:
    split:
      train: 5000
      test: 1000
```

---

## ğŸ” è°ƒè¯•æŠ€å·§

### æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ³¨å†Œ
```python
from engine.datas.loader import DATASET_REGISTRY
print([k for k in DATASET_REGISTRY.keys() if k.startswith("qa-")])
# ['qa-gsm8k', ...]
```

### æ£€æŸ¥æ ·æœ¬æ ¼å¼
```python
sample = train_ds[0]
print(f"Keys: {list(sample.keys())}")
print(f"Question: {sample['question'][:100]}...")
print(f"Final Answer: {sample['final_answer']}")
```

### æµ‹è¯• Judge å‡½æ•°
```python
# æµ‹è¯•å„ç§è¾¹ç•Œæƒ…å†µ
test_cases = [
    ("3", "3", True),
    ("3.0", "3", True),
    ("$3", "3", True),
    ("3.14", "3", False),
]

for pred, ref, expected in test_cases:
    result = judge(pred, ref)
    is_correct = result["correct"] == 1
    assert is_correct == expected, f"Failed: {pred} vs {ref}"
```

---

## ğŸ“š æ›´å¤šèµ„æº

- å®Œæ•´å®ç°æ–‡æ¡£: [QA_DATASET_IMPLEMENTATION.md](QA_DATASET_IMPLEMENTATION.md)
- é¡¹ç›®æ¶æ„æ–‡æ¡£: [CLAUDE.md](CLAUDE.md)
- é…ç½®ç¤ºä¾‹: [configs/example_qa_gsm8k.yaml](configs/example_qa_gsm8k.yaml)
- ç¦»çº¿æµ‹è¯•: `python test_gsm8k_offline.py`
