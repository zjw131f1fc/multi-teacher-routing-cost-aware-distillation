# Step3: 知识蒸馏配置（使用混合数据集：蒸馏训练 + GSM8K 测试）

global_settings:
  seed: 42
  device: "cuda"
  dtype: "bfloat16"
  dataset_cache_dir: "/data/users/zjw/dataset_cache"
  hf_cache_dir: "/data/users/zjw/huggingface_cache"
  study_name: "step3_distill_with_gsm8k"

config_settings:
  enable_dict_overrides: true
  enable_yaml_overrides: true
  log_config_on_load: true

manager_settings:
  name: "basic"
  mode: "direct"
  num_subtasks: 1
  available_gpus: [0]
  gpus_per_subtask: 4
  poll_interval: 5.0

# 混合数据集配置
dataset_settings:
  type: "distill"
  name: "distill-mixed-qa"
  distill_settings:
    split:
      train: 'all'   # 从蒸馏数据集（OpenR1-Math）加载的训练数据
      test: -1       # 占位符（test 数据来自 GSM8K）
    # GSM8K 测试数据配置
    qa_settings:
      split:
        train: -1    # 占位符，不使用 GSM8K 的训练数据
        test: 'all'  # 使用 GSM8K 的全部测试数据（1319 个）

backbone_settings:
  type: "llm"
  name: "qwen2.5-0.5b-instruct"
  model_id: "Qwen/Qwen2.5-0.5B-Instruct"
  llm_settings:
    device_map: "auto"
    attn_implementation: "flash_attention_2"
    torch_dtype: "bfloat16"

trainer_settings:
  type: "dl"
  name: "basic-pytorch"
  dl_settings:
    batch_size: 4
    epochs: 3
    print_loss_every_batches: 50
    eval_every_batches: 500
    save_every_batches: 0
    save_every_epochs: 1
    eval_max_samples: 500
    grad_clip_max_norm: 1.0
    use_warmup: true
    warmup_ratio: 0.1
    optimizers:
      student:
        type: "adamw"
        lr: 5.0e-5
        weight_decay: 0.01

method_settings:
  study_name: "step3_distill_with_gsm8k"
  required_teachers:
    - "deepseek-r1"
    - "qwen2.5-math-7b-instruct"
  routing_strategy: "best_teacher"
  best_teacher_name: "deepseek-r1"
  router_checkpoint: null
  distill_temperature: 1.0
  max_seq_length: 2048
