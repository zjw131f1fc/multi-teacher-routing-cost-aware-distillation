# Step3: 知识蒸馏配置（使用混合数据集：蒸馏训练 + GSM8K 测试）

global_settings:
  seed: 42
  device: "cuda"
  dtype: "bfloat16"
  dataset_cache_dir: "/data/users/zjw/dataset_cache"
  hf_cache_dir: "/data/users/zjw/huggingface_cache"
  study_name: "step3_distill_with_gsm8k"

config_settings:
  enable_dict_overrides: true
  enable_yaml_overrides: true
  log_config_on_load: true

manager_settings:
  name: "basic"
  mode: "direct"
  num_subtasks: 1
  available_gpus: [0]
  gpus_per_subtask: 4
  poll_interval: 5.0

# 多数据集配置（使用新的列表模式）
dataset_settings:
  # 数据集 1: 蒸馏训练数据（OpenR1-Math）
  - type: "distill"
    name: "distill-openr1-math"
    distill_settings:
      split:
        train: 'all'   # 使用所有训练数据
        test: -1       # 不使用蒸馏数据集的测试集
  # 数据集 2: QA 测试数据（GSM8K）
  - type: "qa"
    name: "qa-gsm8k"
    qa_settings:
      split:
        train: -1      # 不使用 GSM8K 的训练数据
        test: 200    # 使用所有测试数据（1319 个）

backbone_settings:
  type: "llm"
  name: "qwen2.5-0.5b-instruct"
  model_id: "Qwen/Qwen2.5-0.5B-Instruct"
  llm_settings:
    device_map: "auto"
    attn_implementation: "flash_attention_2"
    torch_dtype: "bfloat16"

trainer_settings:
  type: "dl"
  name: "basic-pytorch"
  dl_settings:
    batch_size: 2
    epochs: 3
    print_loss_every_batches: 100
    eval_every_batches: 1000
    save_every_batches: 0
    save_every_epochs: 1
    eval_max_samples: 500
    grad_clip_max_norm: 1.0
    use_warmup: true
    warmup_ratio: 0.1
    optimizers:
      student:
        type: "adamw"
        lr: 5.0e-5
        weight_decay: 0.01

method_settings:
  study_name: "step3_distill_with_gsm8k"
  required_teachers:
    - "deepseek-r1"
    # - "qwen2.5-math-7b-instruct"
  routing_strategy: "best_teacher"
  best_teacher_name: "deepseek-r1"
  router_checkpoint: null
  distill_temperature: 1.0
  max_seq_length: 2048
  max_train_length: 2048  # 训练数据的最大长度，超过此长度的样本将被过滤
