# Step2: 训练路由模型配置（使用 DeBERTa Encoder，回归模式）

global_settings:
  seed: 42
  device: "cuda"
  dtype: "float32"  # 使用 FP32 训练（最稳定）
  dataset_cache_dir: "/data/users/zjw/dataset_cache"
  hf_cache_dir: "/data/users/zjw/huggingface_cache"
  study_name: "step2_train_router"

config_settings:
  enable_dict_overrides: true
  enable_yaml_overrides: true
  log_config_on_load: true

manager_settings:
  name: "basic"
  mode: "direct"  # direct模式：单任务直接运行
  num_subtasks: 1
  available_gpus: [0]
  gpus_per_subtask: 4
  poll_interval: 5.0

dataset_settings:
  type: "distill"
  name: "distill-openr1-math"
  distill_settings:
    split:
      train: 'all'   # 从HF加载的训练数据总量
      test: -1       # 占位符，不使用

backbone_settings:
  type: "encoder"
  name: "deberta-v3-base"
  model_id: "microsoft/deberta-v3-base"  # HuggingFace 模型 ID
  encoder_settings:
    device_map: "auto"  # 自动分配设备
    pooling_mode: "mean"  # 使用 mean pooling
    hidden_dim: 768  # DeBERTa-v3-base 的 hidden dimension

trainer_settings:
  type: "dl"
  name: "basic-pytorch"
  dl_settings:
    batch_size: 32              # 批次大小（DeBERTa 比 LLM 小，可以用更大的 batch）
    epochs: 50                  # 训练轮数
    print_loss_every_batches: 10  # 每10个batch打印loss
    eval_every_batches: 50      # 每50个batch评估一次
    save_every_batches: 0      # 0表示不按batch保存
    save_every_epochs: 1       # 每1个epoch保存一次
    eval_max_samples: 0        # 0表示全量评估
    grad_clip_max_norm: 1.0    # 梯度裁剪阈值

    # Learning rate scheduler 配置
    use_warmup: true           # 是否使用 warmup
    warmup_ratio: 0.1          # warmup 占总步数的比例（10%）
    # warmup_steps: 500        # 或者直接指定 warmup 步数（如果设置了则忽略 warmup_ratio）

    # 优化器配置
    optimizers:
      router:
        type: "adamw"
        lr: 2.0e-4  # DeBERTa 微调推荐学习率
        weight_decay: 0.01

method_settings:
  # Study name (用于日志标识和checkpoint保存)
  study_name: "step2_train_router_deberta"

  # 必需的教师列表（需要与 Step1 一致）
  required_teachers:
    - "deepseek-r1"
    - "qwen2.5-math-7b-instruct"

  # 模型参数
  dropout: 0.1               # Dropout rate
