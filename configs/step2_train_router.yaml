# Step2: 训练路由模型配置

global_settings:
  seed: 42
  device: "cuda"
  dtype: "float"
  pytorch_cuda_alloc_conf: "expandable_segments:True"
  dataset_cache_dir: "/data/users/zjw/dataset_cache"
  hf_cache_dir: "/data/users/zjw/huggingface_cache"

config_settings:
  enable_dict_overrides: true
  enable_yaml_overrides: true
  log_config_on_load: true

dataset_settings:
  type: "distill"
  name: "distill-openr1-math"
  distill_settings:
    split:
      train: 10000   # 训练集样本数
      test: 1000     # 测试集样本数

backbone_settings:
  type: "llm"
  name: "qwen2.5-1.5b-instruct"
  model_id: "Qwen/Qwen2.5-1.5B-Instruct"  # HuggingFace 模型 ID
  llm_settings:
    device_map: "auto"

trainer_settings:
  type: "dl"
  name: "pytorch"
  dl_settings:
    batch_size: 8              # 批次大小
    num_epochs: 5              # 训练轮数
    learning_rate: 5.0e-5      # 学习率 (全量微调建议较小)
    weight_decay: 0.01         # 权重衰减
    warmup_ratio: 0.1          # warmup 比例
    gradient_clip: 1.0         # 梯度裁剪
    eval_every_n_epochs: 1     # 每 N 个 epoch 评估一次
    save_every_n_epochs: 1     # 每 N 个 epoch 保存一次

method_settings:
  # Study name (用于日志标识和checkpoint保存)
  study_name: "step2_train_router"

  # 必需的教师列表（需要与 Step1 一致）
  required_teachers:
    - "deepseek-r1"
    - "qwen2.5-math-7b-instruct"

  # 模型参数
  dropout: 0.1               # Dropout rate
  max_seq_length: 512        # 最大序列长度
