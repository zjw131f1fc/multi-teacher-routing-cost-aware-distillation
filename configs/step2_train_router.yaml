# Step2: 训练路由模型配置

global_settings:
  seed: 42
  device: "cuda"
  dtype: "float"  # 使用 FP32 训练（最稳定）
  pytorch_cuda_alloc_conf: "expandable_segments:True"
  dataset_cache_dir: "/data/users/zjw/dataset_cache"
  hf_cache_dir: "/data/users/zjw/huggingface_cache"
  study_name: "step2_train_router"

config_settings:
  enable_dict_overrides: true
  enable_yaml_overrides: true
  log_config_on_load: true

manager_settings:
  name: "basic"
  mode: "direct"  # direct模式：单任务直接运行
  num_subtasks: 1
  available_gpus: [0,1,2,3]
  gpus_per_subtask: 4
  poll_interval: 5.0

dataset_settings:
  type: "distill"
  name: "distill-openr1-math"
  distill_settings:
    split:
      train: 'all'   # 从HF加载的训练数据总量
      test: -1       # 占位符，不使用

backbone_settings:
  type: "llm"
  name: "qwen2.5-1.5b-instruct"
  model_id: "Qwen/Qwen2.5-1.5B-Instruct"  # HuggingFace 模型 ID
  llm_settings:
    device_map: "balanced"  # 使用单个GPU，避免多GPU分布导致的设备不匹配问题

trainer_settings:
  type: "dl"
  name: "basic-pytorch"
  dl_settings:
    batch_size: 8              # 批次大小
    epochs: 5                  # 训练轮数
    print_loss_every_batches: 10  # 每10个batch打印loss
    eval_every_batches: 50      # 0表示不按batch评估
    save_every_batches: 0      # 0表示不按batch保存
    save_every_epochs: 1       # 每1个epoch保存一次
    eval_max_samples: 0        # 0表示全量评估
    grad_clip_max_norm: 1.0    # 梯度裁剪阈值

    # 优化器配置
    optimizers:
      router:
        type: "adam"
        lr: 1.0e-5  # FP32 训练可以用更高的学习率
        weight_decay: 0.01

method_settings:
  # Study name (用于日志标识和checkpoint保存)
  study_name: "step2_train_router"

  # 必需的教师列表（需要与 Step1 一致）
  required_teachers:
    - "deepseek-r1"
    - "qwen2.5-math-7b-instruct"

  # 模型参数
  dropout: 0.1               # Dropout rate
  max_seq_length: 512        # 最大序列长度
