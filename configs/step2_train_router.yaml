# Step2: 训练路由模型配置

global_settings:
  seed: 42
  device: "cuda"
  dtype: "float"  # 使用 FP32 训练（最稳定）
  pytorch_cuda_alloc_conf: "expandable_segments:True"
  dataset_cache_dir: "/data/users/zjw/dataset_cache"
  hf_cache_dir: "/data/users/zjw/huggingface_cache"
  study_name: "step2_train_router"

config_settings:
  enable_dict_overrides: true
  enable_yaml_overrides: true
  log_config_on_load: true

manager_settings:
  name: "basic"
  mode: "direct"  # direct模式：单任务直接运行
  num_subtasks: 1
  available_gpus: [0,1,2,3]
  gpus_per_subtask: 4
  poll_interval: 5.0

dataset_settings:
  type: "distill"
  name: "distill-openr1-math"
  distill_settings:
    split:
      train: 'all'   # 从HF加载的训练数据总量
      test: -1       # 占位符，不使用

backbone_settings:
  type: "llm"
  name: "qwen2.5-1.5b-instruct"
  model_id: "Qwen/Qwen2.5-1.5B-Instruct"  # HuggingFace 模型 ID
  llm_settings:
    device_map: "balanced"  # 使用单个GPU，避免多GPU分布导致的设备不匹配问题

trainer_settings:
  type: "dl"
  name: "basic-pytorch"
  dl_settings:
    batch_size: 16              # 批次大小
    epochs: 40                  # 训练轮数
    print_loss_every_batches: 10  # 每10个batch打印loss
    eval_every_batches: 50      # 0表示不按batch评估
    save_every_batches: 0      # 0表示不按batch保存
    save_every_epochs: 1       # 每1个epoch保存一次
    eval_max_samples: 0        # 0表示全量评估
    grad_clip_max_norm: null    # 梯度裁剪阈值

    # 优化器配置
    optimizers:
      router:
        type: "adam"
        lr: 5.0e-6  # FP32 训练可以用更高的学习率
        weight_decay: 0.01

method_settings:
  # Study name (用于日志标识和checkpoint保存)
  study_name: "step2_train_router"

  # 必需的教师列表（需要与 Step1 一致）
  required_teachers:
    - "deepseek-r1"
    - "qwen2.5-math-7b-instruct"

  # 模型参数
  dropout: 0.3               # Dropout rate
  max_seq_length: 512        # 最大序列长度
  pooling_strategy: "mean"   # Pooling策略: "mean"(平均池化) 或 "last"(最后token)

  # 桶化分类配置
  use_bucketing: false        # 是否使用桶化分类（false则使用回归）
  adaptive_bucketing: true   # 是否使用自适应桶划分（基于K-Means最小化量化误差）

  # 分数差预测模式（新增，仅支持2个教师）
  use_score_diff: true      # 是否使用分数差预测模式（预测强教师分数+弱教师差值）
                             # 注意: use_score_diff 和 use_bucketing 互斥，只能选一个
  strong_teacher_name: "deepseek-r1"  # 强教师的名字（用于分数差模式）
                                       # 必须是 required_teachers 中的一个
                                       # 分数差模式仅支持2个教师！

  # 分桶参数
  num_buckets: 5             # 桶的数量（固定值）
  bucketing_method: "kmeans" # 分桶方法: "kmeans"（量化误差最小）, "quantile"（样本均衡）, "uniform"（等宽桶）
  bucketing_balance_weight: 0.6  # 均匀性权重（0到1之间）
                                  # 0: 只考虑量化误差（默认）
                                  # 1: 只考虑样本分布均匀性
                                  # 0.5: 两者权重相等

  # 自适应分桶配置（当 adaptive_bucketing=true 时使用）

  # 固定桶配置（当 adaptive_bucketing=false 时使用，所有教师共享）
  bucket_ranges: [2.0, 4.0, 6.0, 8.0]  # 固定桶边界（向后兼容）

  # Focal Loss 配置（防止输出趋同）
  focal_gamma: 3.0           # 聚焦参数，越大对易分类样本惩罚越强（0=普通CE，2.0=推荐值，3.0=更强）
