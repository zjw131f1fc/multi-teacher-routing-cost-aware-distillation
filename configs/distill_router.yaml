# Multi-Teacher Routing with Cost-Aware Distillation 配置文件

global_settings:
  seed: 42
  device: "cuda"
  dtype: "float"  # 训练数据类型: "half" (float16) 或 "float" (float32)
  pytorch_cuda_alloc_conf: "expandable_segments:True"
  dataset_cache_dir: "/data/users/zjw/dataset_cache"
  hf_cache_dir: "/data/users/zjw/huggingface_cache"
  save_dir: "./outputs/checkpoints"
  log_dir: "./outputs/logs"
  study_name: "distill_router"

trainer_settings:
  type: "deep-learning"
  name: "basic-pytorch"
  dl_settings:
    epochs: 5
    batch_size: 8
    optimizers:
      router:
        type: "adam"
        lr: 1e-4
      student:
        type: "adam"
        lr: 5e-5
    print_loss_every_batches: 10
    eval_every_batches: 100
    eval_max_samples: 200
    save_every_batches: 500
    save_every_epochs: 1
    grad_clip_max_norm: 1.0

manager_settings:
  name: "basic"
  mode: "direct"
  num_subtasks: 1
  available_gpus: [0]
  gpus_per_subtask: 1
  poll_interval: 5.0

dataset_settings:
  type: "distill"
  name: "distill-openr1-math"
  distill_settings:
    split:
      train: 10000
      test: 1000
    # HuggingFace dataset path (if applicable)
    # hf_path: "open-r1/OpenR1-Math-220k"
    # hf_split: "train"

config_settings:
  enable_dict_overrides: true
  enable_yaml_overrides: true
  log_config_on_load: true

backbone_settings:
  type: "mllm"
  name: "llava-1.5-7b"
  mllm_settings:
    device_map: "auto"
    max_text_tokens: 512
    max_vision_tokens: 576

method_settings:
  # Router settings
  router_d_hidden: 512
  router_dropout: 0.1
  num_teachers: 3  # Number of teacher models in dataset

  # Student settings
  student_use_adapter: false

  # Loss weights
  distill_loss_weight: 1.0
  routing_loss_weight: 0.5
  cost_loss_weight: 0.1

  # Cost-aware routing
  teacher_costs:  # Relative computational costs
    - 1.0  # Teacher 1
    - 2.0  # Teacher 2
    - 3.0  # Teacher 3
  cost_threshold: 2.0  # Maximum average cost target

evaluation_settings:
  eval_metrics:
    - "accuracy"
    - "routing_accuracy"
    - "avg_cost"
