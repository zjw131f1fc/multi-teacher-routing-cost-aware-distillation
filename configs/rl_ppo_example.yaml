# ===== BasicTianshouTrainer示例配置 - PPO算法 =====

# ==================== 全局设置 ====================
global_settings:
  device: "cuda"  # "cuda" 或 "cpu"
  seed: 42

# ==================== Trainer设置 ====================
trainer_settings:
  name: "basic-tianshou"

  rl_settings:
    # ===== 训练类型 =====
    trainer_type: "onpolicy"  # "onpolicy" 或 "offpolicy"

    # ===== 训练参数 =====
    max_epoch: 10
    step_per_epoch: 50000
    batch_size: 64

    # ===== Onpolicy专用参数 =====
    episode_per_collect: 16    # 每次collect多少个episode
    repeat_per_collect: 2      # 每次collect后重复训练多少次

    # ===== 评估设置 =====
    episode_per_test: 10
    test_in_train: true

# ==================== 环境设置 ====================
env_settings:
  name: "CartPole-v1"
  num_train_envs: 10
  num_test_envs: 1
  use_subproc: false

# ==================== Policy设置 ====================
policy_settings:
  # ===== 算法选择 =====
  algorithm: "ppo"  # "dqn", "ppo", "sac", "custom"

  # ===== PPO算法参数 =====
  ppo:
    lr: 0.0003
    discount_factor: 0.99
    gae_lambda: 0.95
    vf_coef: 0.5
    ent_coef: 0.01
    max_grad_norm: 0.5
    eps_clip: 0.2

  # ===== 网络结构（通用） =====
  network:
    hidden_sizes: [64, 64]
    activation: "tanh"

# ==================== Manager设置 ====================
manager_settings:
  mode: "basic"

# ==================== 评估设置 ====================
evaluation_settings:
  custom_eval: false
