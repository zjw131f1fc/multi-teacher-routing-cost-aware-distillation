# Vision Token Pruning with GAN 配置文件

global_settings:
  seed: 42
  device: "cuda"
  dtype: "float"  # 训练数据类型: "half" (float16) 或 "float" (float32)
  pytorch_cuda_alloc_conf: "expandable_segments:True"
  dataset_cache_dir: "/data/users/zjw/dataset_cache"
  hf_cache_dir: "/data/users/zjw/huggingface_cache"
  save_dir: "./outputs/checkpoints"
  log_dir: "./outputs/logs"
  study_name: "gan_vtp"  # Vision Token Pruning研究名称

trainer_settings:  # 训练框架设置
  type: "deep-learning"
  name: "basic-pytorch"
  dl_settings:
    epochs: 1  # 2个epochs
    batch_size: 10  # batch_size=8
    optimizers:
      discriminator:  # Discriminator优化器
        type: "adam"
        lr: "3.2e-04"  # 最优值
      token_merger:  # Token Merger优化器（可以设置更高的学习率）
        type: "adam"
        lr: "1e-05"  # 默认1e-3，比layer_pruners高10倍
      layer_pruners:  # Layer Pruners优化器
        type: "adam"
        lr: "1.7e-05"  # 最优值
    print_loss_every_batches: 20  # 减少日志打印
    eval_every_batches: 100  # 每25个batch评估一次
    eval_max_samples: 50  # 每次评估50个样本
    save_every_batches: 500  # Optuna搜索时禁用保存（加速）
    save_every_epochs: 0  # Optuna搜索时禁用保存
    optuna_eval_interval_batches: 40  # Optuna评估间隔：每25个batch报告一次
    grad_clip_max_norm: 1.0  # 梯度裁剪：null=关闭，1.0=启用

manager_settings:
  name: "basic"
  mode: "direct"  # null=单任务模式, "optuna"=超参数搜索, "batch_configs"=批量配置, "direct"=直接运行
  num_subtasks: 1  # 单任务：每个trial需要3个GPU，5张卡只够开1个
  available_gpus: [1,3,5]  # 更新可用GPU列表
  gpus_per_subtask: 3  # 每个trial使用3个GPU
  poll_interval: 5.0  # 轮询间隔，减少CPU占用

search_settings:
  enable: false
  type: "optuna"
  n_trials: 200  # 减少trial数量（单worker串行执行）
  study_name: "attn_bias_search"
  pruner:
    type: "successive_halving"
    min_resource: 2  # 最少跑2个评估点才能被剪枝
    reduction_factor: 3
    min_early_stopping_rate: 0
  sampler:
    type: "tpe"
    n_startup_trials: 15  # TPE warmup：前10个trial随机采样
    multivariate: true  # 考虑参数间相关性
  params:
    # 搜索范围：基于最优参数上下两个数量级
    # 最优参数来源: best_params.json (trial)

    # Layer Pruners 学习率 (最优: 1.27e-4)
    trainer_settings.dl_settings.optimizers.layer_pruners.lr:
      type: "float"
      low: 1.0e-6   # 1.27e-4 / 100
      high: 1.0e-2  # 1.27e-4 * 100
      log: true

    # Discriminator 学习率 (最优: 1.30e-4)
    trainer_settings.dl_settings.optimizers.discriminator.lr:
      type: "float"
      low: 1.0e-6   # 1.30e-4 / 100
      high: 1.0e-2  # 1.30e-4 * 100
      log: true

    # Loss权重（基于最优参数上下两个数量级）
    # adv_loss_weight (最优: 29.1)
    method_settings.adv_loss_weight:
      type: "float"
      low: 0.3      # 29.1 / 100
      high: 3000.0  # 29.1 * 100
      log: true

    # task_loss_weight (最优: 3.87)
    method_settings.task_loss_weight:
      type: "float"
      low: 0.04     # 3.87 / 100
      high: 400.0   # 3.87 * 100
      log: true

    # sparsity_weight (最优: 0.212)
    method_settings.sparsity_weight:
      type: "float"
      low: 0.001    # 0.212 / 100
      high: 100.0    # 0.212 * 100
      log: true

    # token_count_loss_weight (最优: 7.02)
    method_settings.token_count_loss_weight:
      type: "float"
      low: 0.001    # 7.02 / 100
      high: 100.0   # 7.02 * 100
      log: true

    # binarization_loss_weight: 鼓励soft_mask接近0或1
    method_settings.binarization_loss_weight:
      type: "float"
      low: 0.001
      high: 100.0
      log: true

    # Discriminator配置（线性搜索，±0.1范围）
    # disc_reinit_prob (最优: 0.084)
    method_settings.disc_reinit_prob:
      type: "float"
      low: 0.0      # max(0, 0.084 - 0.1)
      high: 0.18    # 0.084 + 0.1
      log: false

    # disc_dropout (最优: 0.115)
    method_settings.disc_dropout:
      type: "float"
      low: 0.02     # max(0, 0.115 - 0.1)
      high: 0.22    # 0.115 + 0.1
      log: false

    # Attention Bias相关参数（新增）
    # use_attn_residual: 是否启用attention residual
    method_settings.use_attn_residual:
      type: "bool"

    # learnable_attn_weight: attention weight是否可学习
    method_settings.learnable_attn_weight:
      type: "bool"

    # ==================== Gumbel Softmax 相关参数 ====================
    # temperature: 初始temperature（控制soft/hard程度，典型值0.5-2.0）
    method_settings.temperature:
      type: "float"
      low: 0.5
      high: 2.0
      log: false

    # temperature_min: 最小temperature（annealing终点，典型值0.05-0.5）
    method_settings.temperature_min:
      type: "float"
      low: 0.05
      high: 0.5
      log: false

    # temperature_anneal_rate: Anneal比例（前X%步数进行annealing，0.3-0.8）
    method_settings.temperature_anneal_rate:
      type: "float"
      low: 0.1
      high: 0.8
      log: false

dataset_settings:  # 数据集配置
  name: "vqa-vqav2"  # 数据集名称
  split:
    train: 14000  # 训练集：800样本（充分训练）
    test: 200  # 测试集：200样本（准确评估）
  category_priority:
    enable: false  # 禁用类别均衡，加快数据加载
    values:
      - train: "mean"
      - test: "mean"
  fast_load_no_random: true  # 快速加载模式（Optuna搜索时加速）

config_settings:
  enable_dict_overrides: true
  enable_yaml_overrides: true
  log_config_on_load: true

backbone_settings:  # 多模态大模型骨架配置
  type: "mllm"  # 骨架类型
  name: "llava-1.5-7b"  # 模型名称
# name: "llava-1.5-7b" "qwen-2.5-3b"
  mllm_settings:
    device_map: "balanced"  # 显存分配策略（balanced/auto）
    max_text_tokens: 128  # 最大文本token数
    max_vision_tokens: 1600  # 最大视觉token数（减少降低显存占用）
    image_max_size: 1100  # 图像最大尺寸（减少降低vision token数）


method_settings:
  # ==================== Token Merger配置（新增） ====================
  enable_token_merger: false  # 是否启用token merger（true=启用, false=禁用）
  merger_type: "fixed_pooling"  # "simple" / "question_aware" / "fixed_pooling" (推荐V3)
  merge_ratio: 1.0  # 保留比例
  merger_dropout: 0.1  # Token Merger 的 dropout（防过拟合）
  merger_d_internal: 512  # Merger内部维度

  # ==================== Layer-wise Pruner配置（新增） ====================
  pruning_layers: [5,15,25]  # 要剪枝的LLM层索引
  pruner_d_internal: 512  # Pruner内部维度
  pruner_num_heads: 4  # Cross-attention头数
  pruner_type: "cross_attention"  # "cross_attention" 或 "simple"
  pruner_dropout: 0.1  # Layer Pruner 的 dropout（防过拟合）

  # Attention Residual配置（新功能）
  use_attn_residual: false  # 是否启用attention residual（残差连接text→vision attention）
  attn_residual_weight: 0.5  # Residual权重（固定值或初始值）
  learnable_attn_weight: true  # Residual weight是否可学习（true=可学习参数，false=固定值）

  # ==================== 剪枝目标配置 ====================
  use_token_num_target: true  # true=使用绝对token数目标, false=使用稀疏度比例
  target_token_num: 200  # 目标保留的绝对token数（use_token_num_target=true时生效）
  target_sparsity: 0.3  # 目标稀疏度（use_token_num_target=false时生效，0.3表示剪掉30%）

  # ==================== Sparsity Loss配置 ====================
  sparsity_loss_only_on_excess: true  # true=只在超出目标时惩罚（强约束）, false=双向惩罚
  sparsity_weight: 10.67  # Sparsity约束loss权重（强惩罚）
  token_count_loss_weight: 0.16  # Token总数loss权重（弱惩罚，鼓励减少token）
  binarization_loss_weight: 0.41  # Binarization loss权重，鼓励soft_mask接近0或1

  # ==================== Temperature Annealing（新增） ====================
  temperature: 0.99  # 初始temperature
  temperature_min: 0.5  # 最小temperature
  temperature_anneal_rate: 0.4  # Anneal比例（前50%步数进行annealing）

  # ==================== Hard Pruning配置（评估时使用） ====================
  hard_pruning_threshold: 0.5  # Hard剪枝阈值（soft_mask > threshold则保留）

  # Discriminator配置
  disc_num_layers: 3  # 使用的LLM隐层数量
  disc_d_d: 512  # 隐层维度
  disc_dropout: 0.2  # Dropout比率
  disc_target_layers: [-1, -3, -5]  # 目标LLM层索引
  disc_reinit_prob: 0.3  # 每个batch重初始化Discriminator的概率
  disc_use_spectral_norm: false  # 谱归一化（GAN稳定性）

  # ==================== 损失权重（动态调度） ====================
  # 余弦调度策略：训练初期优先任务性能，后期强化对抗训练

  # Task Loss权重（递减）
  task_loss_weight_start: 80.0  # 初始权重（高优先级，优先学习保留信息）
  task_loss_weight: 20  # 目标权重（warmup后的稳定值）

  # Adversarial Loss权重（递增）
  adv_loss_weight_start: 1.0  # 初始权重（低对抗，避免过早压制pruner）
  adv_loss_weight: 40  # 目标权重（warmup后强化对抗）

  # Warmup配置
  loss_weight_warmup_ratio: 0.3  # 前30%步数进行warmup（0.0=关闭动态调度）

evaluation_settings:  # 评估配置
  eval_mode: ["origin", "hard"]  # origin=无剪枝, merge_only=只merge, soft=软剪枝, hard=硬剪枝
